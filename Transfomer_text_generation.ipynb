{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is Text Generation?\n",
        "\n",
        "Text generation is the task of predicting the **next word/token** given previous text.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Chatbots\n",
        "* Code generation\n",
        "* Story writing\n",
        "* Question answering\n",
        "* Autocomplete & smart replies\n",
        "\n",
        "\n",
        "> **Given some text → predict what comes next**"
      ],
      "metadata": {
        "id": "KNWjEMpLNGY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Traditional Approaches\n",
        "\n",
        "### N-grams\n",
        "\n",
        "* Look at last *n* words\n",
        "* Fails for long context\n",
        "\n",
        "### RNNs / LSTMs\n",
        "\n",
        "* Process text sequentially\n",
        "* Capture context better\n",
        "* ❌ Slow to train\n",
        "* ❌ Struggle with very long sequences\n",
        "\n",
        "**Transformers solve these problems**"
      ],
      "metadata": {
        "id": "onPTs8wh-yxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Why Transformers? (Key Motivation)\n",
        "\n",
        "Transformers were introduced in the paper:\n",
        "\n",
        "> *\"Attention Is All You Need\"* (2017)\n",
        "\n",
        "### Problems with RNNs:\n",
        "\n",
        "* Sequential processing → slow\n",
        "* Long-range dependencies fade over time.\n",
        "\n",
        "### Transformers:\n",
        "\n",
        "* Process **all words in parallel**\n",
        "* Use **attention** to focus on important words"
      ],
      "metadata": {
        "id": "VxvYDaB5-0lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. High-Level Transformer Architecture\n",
        "\n",
        "A Transformer consists of:\n",
        "\n",
        "1. **Embedding Layer**\n",
        "2. **Positional Encoding**\n",
        "3. **Self-Attention**\n",
        "4. **Feed Forward Network**\n",
        "5. **Layer Normalization & Residuals**\n",
        "6. **Output Softmax**\n",
        "\n",
        "For **text generation**, we use **Decoder-only Transformers**."
      ],
      "metadata": {
        "id": "hIgWej6i-9v3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Tokenization\n",
        "\n",
        "Neural networks dont understand text, they understand only numbers. So, to chnage it into vector represnetation we first divide them inti individual tokens.\n",
        "\n",
        "### Tokenization:\n",
        "\n",
        "\"I love AI\" → `[\"I\", \"love\", \"AI\"]`\n"
      ],
      "metadata": {
        "id": "gwGKg1YK_Bh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Embeddings\n",
        "\n",
        "Each token is converted into a **vector**.\n",
        "\n",
        "Why?\n",
        "\n",
        "* Similar words → similar vectors\n",
        "* Captures semantic meaning\n",
        "\n",
        "Example:\n",
        "\n",
        "* king - man + woman ≈ queen"
      ],
      "metadata": {
        "id": "vw3WZXz1_GxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Positional Encoding\n",
        "\n",
        "Each word is processed parallely ,so transformers **do not know word order** by default.\n",
        "\n",
        "\n",
        "\"Dog bites man\" ≠ \"Man bites dog\"\n",
        "\n",
        "So we add **position information** to embeddings.\n",
        "\n",
        "> This helps the model understand sequence order."
      ],
      "metadata": {
        "id": "ss90mlyQ_MNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Attention — The Core Idea\n",
        "\n",
        "When predicting a word, not all previous words are equally important.\n",
        "\n",
        "Example:\n",
        "\n",
        "> \"The capital of France is ___\"\n",
        "\n",
        "The model should **pay more attention** to:\n",
        "\n",
        "* capital\n",
        "* France\n",
        "\n",
        "and less to:\n",
        "\n",
        "* the\n",
        "* of"
      ],
      "metadata": {
        "id": "mIbPh-PI_Rij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Self-Attention\n",
        "\n",
        "In **self-attention**, each word:\n",
        "\n",
        "* Looks at **all other words**\n",
        "* Decides how much to focus on each\n",
        "\n",
        "This allows:\n",
        "\n",
        "* Long-range dependencies\n",
        "* Parallel computation"
      ],
      "metadata": {
        "id": "dvlp_2gb_WOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Multi-Head Attention\n",
        "\n",
        "Instead of one attention mechanism:\n",
        "\n",
        "* Multiple attention heads run in parallel\n",
        "* Each head focuses on different relationships\n",
        "\n",
        "Example:\n",
        "\n",
        "* Head 1 → grammar\n",
        "* Head 2 → meaning\n",
        "* Head 3 → entities"
      ],
      "metadata": {
        "id": "ubT44OT5_ZrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Feed Forward Network\n",
        "\n",
        "After attention:\n",
        "\n",
        "* Each token passes through a small neural network\n",
        "* Adds non-linearity\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> \"Refining\" the attended information"
      ],
      "metadata": {
        "id": "5S_VLnX2_eFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Decoder-Only Transformers\n",
        "\n",
        "For text generation, we use:\n",
        "\n",
        "* **Masked self-attention**\n",
        "* Model can only see **previous tokens**, not future ones\n",
        "\n",
        "This prevents cheating during training."
      ],
      "metadata": {
        "id": "_ECoP0Bm_vx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. How Text Generation Works\n",
        "\n",
        "### Training:\n",
        "\n",
        "Input:\n",
        "\n",
        "\"I love deep\" → Predict \"learning\"\n",
        "\n",
        "Loss:\n",
        "\n",
        "* Cross-entropy loss\n",
        "\n",
        "### Inference:\n",
        "\n",
        "1. Start with prompt\n",
        "2. Predict next token\n",
        "3. Append token\n",
        "4. Repeat\n"
      ],
      "metadata": {
        "id": "w_ZV8RJ__1pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. Popular Transformer Models\n",
        "\n",
        "* GPT-2 / GPT-3 / GPT-4\n",
        "* BERT (not for generation)\n",
        "* T5\n",
        "* LLaMA"
      ],
      "metadata": {
        "id": "fxvfCzsD_6Do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRACTICAL SECTION"
      ],
      "metadata": {
        "id": "UmBMnU1R_7ZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Setup"
      ],
      "metadata": {
        "id": "skxr7PXjNWcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pifqxrrwNMgH",
        "outputId": "a480ec37-c10c-4061-bf53-f0d7fdc10c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Load Model"
      ],
      "metadata": {
        "id": "nzHgPpYANZTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "KMbSZsJYNd_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Examples"
      ],
      "metadata": {
        "id": "bwhMLqTQNv30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Artificial intelligence will\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "outputs = model.generate(\n",
        "**inputs,\n",
        "max_length=50,\n",
        "do_sample=True,\n",
        "temperature=0.7\n",
        ")\n",
        "\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR-Irwn8NvC7",
        "outputId": "de284403-2e87-4449-a59b-e056c917287b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence will never be as simple as the humans may want it to be, but the possibilities are endless. It will change the way we interact with machines, and, like everything else in life, we will need to change it.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Important Generation Parameters\n",
        "**max_length**\n",
        "\n",
        "\n",
        "*   Maximum tokens generated\n",
        "\n",
        "\n",
        "**temperature**\n",
        "\n",
        "Controls randomness\n",
        "\n",
        "*   Low → deterministic\n",
        "*   High → creative\n",
        "\n",
        "\n",
        "**top_k / top_p**\n",
        "\n",
        "\n",
        "*   Limits vocabulary choices\n",
        "*   Prevents nonsense text"
      ],
      "metadata": {
        "id": "4-JBeGsdOIze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Controlled Generation Example"
      ],
      "metadata": {
        "id": "ULBzxvO5OsQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "**inputs,\n",
        "max_length=60,\n",
        "do_sample=True,\n",
        "temperature=0.8,\n",
        "top_k=50,\n",
        "top_p=0.95\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYW4zw9NOyGO",
        "outputId": "5c0bffdd-74c1-4e94-b7bf-0355a385d53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence will do some more of this.\n",
            "\n",
            "\"The AI system will be able to detect where people are when they're around and will be able to make more intelligent decisions,\" he said.\n",
            "\n",
            "\"It will also be able to help people get better at jobs.\n",
            "\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20. Why Transformers Work So Well\n",
        "\n",
        "* Parallel processing\n",
        "* Strong contextual understanding\n",
        "* Scales with data\n",
        "* Flexible architecture\n"
      ],
      "metadata": {
        "id": "ibnX1Hj4NBN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 21. Limitations\n",
        "\n",
        "* Large models = high compute\n",
        "* Can hallucinate\n",
        "* Sensitive to prompts"
      ],
      "metadata": {
        "id": "XO0pgeJGAQ9D"
      }
    }
  ]
}